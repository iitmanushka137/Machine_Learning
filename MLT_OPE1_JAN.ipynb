{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLT_OPE1_JAN.ipynb","provenance":[],"authorship_tag":"ABX9TyNme+zcG1qm9Rbm87VYwzoO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import random\n","import itertools\n","import functools"],"metadata":{"id":"YlwXEJ9SgcNp","executionInfo":{"status":"ok","timestamp":1656047158646,"user_tz":-330,"elapsed":5,"user":{"displayName":"ANUSHKA KRISHNA","userId":"07403897069686417463"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["He uses this data to build a linear regression model from scratch using numpy. He starts with a estimated model given by the equation:\n","\n","crop yield = 15 +4.5 x (amount of water) + 25 x (amount of fertilizer)\n","\n","He uses gradient decent for optimization of his model.\n","\n","Write a function loss(X,w,y) which takes feature matrix(X), weight vector(w) and output label vector(y) and returns sum squared loss while implementing regression model. (Note: Do necessary preprocessing of X)\n"],"metadata":{"id":"of5eUbLFdErJ"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"OiPVHCHgcbPx","executionInfo":{"status":"ok","timestamp":1656047161353,"user_tz":-330,"elapsed":4,"user":{"displayName":"ANUSHKA KRISHNA","userId":"07403897069686417463"}}},"outputs":[],"source":["def loss(X,w,y):\n","  X = np.column_stack((np.ones(X.shape[0]),X))\n","  if X.shape[-1]!=w.shape[0]:\n","    return None\n","  e = (X@w)-y\n","  sse = 0.5*(e@e.T)\n","  return sse"]},{"cell_type":"markdown","source":["He starts with a estimated model given by the equation:\n","\n","crop yield = 15+ 4.5 x (amount of water) + 25 x (amount of fertilizer)\n","\n","He uses gradient decent for optimization of his model.\n","\n","Write a function weight_update(X,w,y,lr) to get updated weight after one iteration of gradient descent given that X is the feature matrix, w is the weight vector and y is the output label vector and Ir is the lear\n","\n","rate.\n","\n","(Note: Do necessary preprocessing of X)"],"metadata":{"id":"n7B7ecNSdPmf"}},{"cell_type":"code","source":["def weight_update(X,w,y,lr):\n","  X = np.column_stack((np.ones(X.shape[0]),X))\n","  if X.shape[-1]!=w.shape[0]:\n","    return None\n","  e = (X@w) - y\n","  G = X.T@e\n","  w_new = w - lr*G\n","  return w_new"],"metadata":{"id":"cw3YYB9FdQ4_","executionInfo":{"status":"ok","timestamp":1656047164188,"user_tz":-330,"elapsed":4,"user":{"displayName":"ANUSHKA KRISHNA","userId":"07403897069686417463"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Consider input feature feature_matrix X of shape (500 x 1) and the degree of the polynomial is a random\n","\n","number (between 2 to 10).\n","\n","Write a function named 'polynomial_transform' that takes X and degree as inputs and returns transformed\n","\n","features.\n","\n","The output of the function should be a numpy array."],"metadata":{"id":"04BCFqMAdRad"}},{"cell_type":"code","source":["def get_combinations(x,degree):\n","    return itertools.combinations_with_replacement(x,degree)\n","    \n","def compute_new_feature(i):\n","    return functools.reduce(lambda x, y:x*y,i)\n","\n","\n","def polynomial_transform(x, degree):\n","    if x.ndim==1:\n","        x = x[:,None]\n","    features = [np.ones(len(x))]\n","    \n","    for d in range(1,degree+1):\n","        for i in get_combinations(x.T,d):\n","            features.append(compute_new_feature(i))\n","    return(np.asarray(features).transpose())"],"metadata":{"id":"e2ePhzmidgEj","executionInfo":{"status":"ok","timestamp":1656047167051,"user_tz":-330,"elapsed":4,"user":{"displayName":"ANUSHKA KRISHNA","userId":"07403897069686417463"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Write a function OneHotEncode(y) to convert string labels to one hot-encoded labels.\n","\n","A column is assigned to a label according to its lexicographically sorted order.\n","\n","Input: a label vector of shape (n,1) where n is number of samples.\n","\n","Output: one hot encoded matrix of shape (n,k) where k is the number of unique labels.\n","\n","Example:\n","\n","Input:\n","$y$ = ['Zebra','Cat','Apple']T\n","output :\n","$Y$ = <br>\n","      [0 0 1<br>\n","      0 1 0<br>\n","      1 0 0]\n","\n"],"metadata":{"id":"C__EAaYidgbe"}},{"cell_type":"code","source":[""],"metadata":{"id":"_DuSrBPLfVzK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write a function Classification Metrics (y_true, y_predicted) that computes accuracy, precision, recall and F1 score from true labels vector and predicted label vector.\n","\n","There are only two possible labels +1 and -1 for positive and negative classes only.\n","\n","Input: Two column vectors of shape (n,1) representing true and predicted labels respectively.\n","\n","Output: A row vector with shape (4,). The four elements are respectively.\n"],"metadata":{"id":"7Lfbxh0_fWMx"}},{"cell_type":"code","source":["def Classifications_Metrics(y_true, y_predicted):\n","  TP = 0\n","  TN = 0\n","  FP = 0\n","  FN = 0\n","  for i in range(len(y_true)):\n","    if y_true == y_predicted:\n","      if y_true == 1:\n","        TP+=1\n","      else:\n","        TN+=1\n","    else:\n","      if y_true ==1:\n","        FN+=1\n","      else:\n","        FP+=1\n","  accuracy = (TP+TN)/(TP+TN+FP+FN)\n","  precision = TP/(TP+FP)\n","  recall = TP/(TP+FN)\n","  f1_score = (2*precision*recall)/(precision + recall)\n","  ans = [accuracy, precision, recall, f1_score]\n","  return ans"],"metadata":{"id":"LXZy_fq_fdOq","executionInfo":{"status":"ok","timestamp":1656047182720,"user_tz":-330,"elapsed":572,"user":{"displayName":"ANUSHKA KRISHNA","userId":"07403897069686417463"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["He uses this data to build a linear regression model from scratch using numpy.\n","\n","He starts with a estimated model given by the equation:\n","\n","crop yield = 15+ 4.5 x (amount of water) + 25 x (amount of fertilizer)\n","\n","Write a function add_dummy(X) to add dummy feature as first column to this feature matrix X."],"metadata":{"id":"-faGrdiSfdja"}},{"cell_type":"code","source":["def add_dummy(X):\n","  X = np.column_stack((np.ones(X.shape[0]), X))\n","  return X"],"metadata":{"id":"FhV6PAetflyX","executionInfo":{"status":"ok","timestamp":1656047248436,"user_tz":-330,"elapsed":5,"user":{"displayName":"ANUSHKA KRISHNA","userId":"07403897069686417463"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Write a function CountPositivePredictions(M, p) which counts number of perceptron models (represented by M) that classify point pa\n","\n","Use sign function for perceptron model.\n","\n","Input:\n","\n","M: The matrix M represents a set of perceptron model.\n","\n","Each row of M is of the format [wo, wi, w2].\n","\n","The shape of M is (q,3) where 'q' is the number of perceptron models.\n","\n","p: a column vector of shape (2, 1).\n","\n","Output: ans, where 0 ans q. ans' is count of models that classify 'p' as positive."],"metadata":{"id":"1X9_bWsxfmUO"}},{"cell_type":"code","source":["def CountPositivePerceptron(M,p):\n","  "],"metadata":{"id":"efKtoLAlfuaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You are provided with an input feature matrix X, label y and weight w. The size of X, y and w are fixed to (500 x 50), (500 x 1) and (50 x 1).\n","\n","You do not need to add a dummy feature to X.\n","\n","Write a loss function named 'loss' that computes ridge and lasso regression losses for the given data.\n","\n","This function should take the feature matrix, label vector, weight vector and regularization rate as inputs and return both the loss values as a list - [ridge loss, lasso loss]."],"metadata":{"id":"a0l4Fj7gfuyX"}},{"cell_type":"code","source":["def loss(X,y,w,lr):\n","  if X.shape[-1]!=w.shape[0]:\n","    return None\n","  e = (X@w)-y\n","  ridge = 0.5*(e@e.T) + (lr/2)*(w.T@w)\n","  lasso = 0.5*(e@e.T) + 0.5*lr*np.sum(abs(w))\n","  result = [ridge, lasso]\n","  return result"],"metadata":{"id":"ElphdQ2Wf2So","executionInfo":{"status":"ok","timestamp":1656047596906,"user_tz":-330,"elapsed":7,"user":{"displayName":"ANUSHKA KRISHNA","userId":"07403897069686417463"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["After plotting the data he identifies that there exists a non-linear relationship between X and Y.\n","\n","He wants to develop a polynomial regression model through which he wants to identify the degree of the polynomial which is best su\n","\n","data.\n","\n","Do the following:\n","\n","- Apply polynomial transformation with degrees 2 to 5 on X.\n","\n","- Using only numpy, implement a linear regression model by gradient descent procedure on the transformed data with 50 iterations, in and learning rate as 0.01 for each degree of polynomial transformation.\n","\n","Define a function named 'compute_error to compute the training mean squared errors for each model and return the degree of the m minimum error.\n","\n","1. Input: X (feature vector: numpy array), Y (label vector: numpy array), Ir (learning rate: float)\n","\n","2. Output: (degree: integer)"],"metadata":{"id":"XumOQAW3f10s"}},{"cell_type":"code","source":["def get_combinations(x,degree):\n","    return itertools.combinations_with_replacement(x,degree)\n","    \n","def compute_new_feature(i):\n","    return functools.reduce(lambda x, y:x*y,i)\n","\n","\n","def polynomial_transform(x):\n","    if x.ndim==1:\n","        x = x[:,None]\n","    features = [np.ones(len(x))]\n","    \n","    for d in range(2,6):\n","        for i in get_combinations(x.T,d):\n","            features.append(compute_new_feature(i))\n","    return(np.asarray(features).transpose())\n","    \n","class linreg() :  \n","    def __init__(self,x,y):\n","        self.x = x\n","        self.y = y\n","        \n","    def gd(self):\n","        from random import randint\n","        w = np.zeros(self.x.shape[-1])\n","        for epoch in range(300):\n","            for i in range(self.x.shape[0]):\n","                index = randint(0,self.x.shape[0]-1)\n","                x_i = self.x[index:index+1]\n","                y_i = self.y[index:index+1]\n","                gradient = (x_i.T) @ ((x_i @ w) - y_i)\n","                w -= 0.001*gradient\n","        return w\n","\n","def output(x,y,test):\n","    out = None    \n","    features = polynomial_transform(x, 2)\n","    lin_reg = linreg(features, y)\n","    w = lin_reg.sgd()\n","    test_feature = polynomial_transform(test, 2)\n","    out = test_feature @ w\n","    return out\n"],"metadata":{"id":"rxqjq2MigH8Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"a0WfE62cgIW1"}}]}