{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of MLT_week_2.ipynb","provenance":[{"file_id":"12wLZ16y5LPBF37oATnjMItbOB2Th1OMx","timestamp":1656031342901},{"file_id":"1kbsA39qtAOpZTo9h0MWG5YtuRSrIKLxj","timestamp":1655781253228},{"file_id":"https://gist.github.com/CKPIITM/02bb106785e00f608b6073517f9ba801#file-mlt_week_2-ipynb","timestamp":1655477510816}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Implement the Model inference in vectorized form "],"metadata":{"id":"zFJWk6zBDT0B"}},{"cell_type":"markdown","source":["The objective of this colab is \n","* Demonstrate training data and model components of linear regression model.\n","* Demonstrate how simple it is to implement ML components and algorithms.\n","\n","Let's first import necessary libraries\n"],"metadata":{"id":"vFbZRXtU9bKP"}},{"cell_type":"code","source":["from IPython.display import display, Math, Latex # Imported for proper rendering of latex in colab.\n","import numpy as np\n","# Import for generating plots\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"],"metadata":{"id":"U5I0N2rA9ycq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quick recap\n","1. Traning data contains features and label that is real number.\n","2. Model or inference: **y=Xw**\n","\n","#C1: Traning Data\n"],"metadata":{"id":"PxgV5Q2t-MMW"}},{"cell_type":"code","source":["#Create a dataset of 100 examples with a single feature and a label.\n","#For this construction, we use the following three parameters:\n","w1=3\n","w0=4\n","n=100\n","x=10*np.random.rand(n,)"],"metadata":{"id":"BvDLBIHG-gfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X=10*np.random.rand(100)\n"],"metadata":{"id":"VjbLuq0w_GO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Obtain y =4 + 3*x + noise . Noise is randomly sampled.\n","y=w0 + w1 *X + np.random.rand(n,)"],"metadata":{"id":"qz_nlR3T_efM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's examine the shapes of training data for sanity check\n"],"metadata":{"id":"ZsfP-tN8LDcs"}},{"cell_type":"code","source":["print('Shape of the training data feature matrix:', X.shape)\n","print('Shape of the label vector:', y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jjKYnrcILMYU","outputId":"b1cf9428-66f0-4690-8303-bc1ead487048"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the training data feature matrix: (100,)\n","Shape of the label vector: (100,)\n"]}]},{"cell_type":"markdown","source":["Let's divide the data into training and test set. We will set aside 20% examples for testing."],"metadata":{"id":"3_AXbXhPLieH"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state=42)"],"metadata":{"id":"hXPzZOi9Ltlj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's do a quick sanity check to make sure the sizes of feature and lables set are identical both in training and test sets:\n"],"metadata":{"id":"7kMgunZdML2n"}},{"cell_type":"code","source":["print(\"Shape of training features matrix:\", X_train.shape)\n","print(\"Shape of training features matrix:\", y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8GSF6l1MUfU","outputId":"1c4f6108-a600-4513-923c-f9da9828db21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of training features matrix: (80,)\n","Shape of training features matrix: (80,)\n"]}]},{"cell_type":"code","source":["print('shape of test feature matrix:', X_test.shape)\n","print('shape of test feature matrix:', y_test.shape)"],"metadata":{"id":"KQUFF1fNN8F9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e4099ff-be66-4045-80f6-ceabae1cc853"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["shape of test feature matrix: (20,)\n","shape of test feature matrix: (20,)\n"]}]},{"cell_type":"markdown","source":["let's quickly check the first few examples and labels\n"],"metadata":{"id":"5edWaf-XOYJe"}},{"cell_type":"code","source":["X_train[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ihu3C3HOiAY","outputId":"6468652b-5505-4b63-d59b-7a258ff43d0a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2.3524671 , 7.49300262, 1.89967955, 0.66130858, 9.34204539])"]},"metadata":{},"execution_count":185}]},{"cell_type":"code","source":["y_train[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wb7w1_fIOqiq","outputId":"0e9bf1cb-4a81-4f40-e5bc-74d0e098e476"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([11.24311479, 27.26303657, 10.31362346,  6.10859111, 32.55319639])"]},"metadata":{},"execution_count":186}]},{"cell_type":"markdown","source":["Let's visualize the training set."],"metadata":{"id":"s3fcszJlOtuj"}},{"cell_type":"code","source":["sns.set_style(\"white\")\n","f=plt.figure(figsize=(8,8))\n","sns.set_context(\"notebook\",font_scale=1.5, rc={\"lines.linewidth\":2.5})\n","\n","plt.plot(X_train,y_train, \"b.\")\n","plt.title(\"Data Points\")\n","plt.grid(True)\n","plt.xlabel(\"$x_1$\",fontsize=18)\n","plt.ylabel(\"$y$\",rotation=90,fontsize=18)\n","plt.axis([0,10,0,40])\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"Vjn0EtejO5u9","outputId":"5c0ec483-3753-4315-ce0c-7900fe0267e6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 1 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","source":["We have a training set consisting a single feature so we will fit a simple linear regression model with one feature. It's form is $y=w_0 + w_1 x_1$.\n","As discussed in the lectures, we add a special dummy feature $x_0$ and set it to 1. We create a helper function for that."],"metadata":{"id":"8U9kNeryQPS8"}},{"cell_type":"code","source":["def add_dummy_feature(x):\n","  ''' Adds a dummy feature to the dataset.\n","\n","  Args:\n","     x: Training dataset\n","  Returns:\n","     Training dataset with an addition of dummy feature.\n","  ''' \n","  #np.ones(x.shape[0]) create a vector of 1's having the same number of rows as number of samples in dataset.\n","  return np.column_stack((np.ones(x.shape[0]),x)) \n","  "],"metadata":{"id":"eoE50jRNQ4pe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's write a test case to test this function:\n","For that let's take two examples and three features. The first example is a feature vector:\n","\\begin{equation}\n","X_{3 \\times 1} ^{(1)} = \\begin{bmatrix} \n","3\\\\2\\\\5\\\\ \\end{bmatrix} \\end{equation}\n","\n","and the seccond example is:\n","\\begin{equation} \\textbf X_{3\\times 1} ^{(2)} = \\begin{bmatrix} 9\\\\4\\\\7 \\end{bmatrix} \\end{equation} \n","\n","And recall that a feature matrix $\\textbf X$ has shape $(n,m)$ corresponding to features of all examples before adding the dummy feature $x_0$.\n","\n","\\begin{equation}\n","X_{n\\times m}  = \\begin{bmatrix} \n","-(\\textbf x^{(1)})^{T} -\\\\-(\\textbf x^{(2)})^{T} -\\\\ \\vdots\\\\-(\\textbf x^{(n)})^{T} -\\\\ \\end{bmatrix} \\end{equation}\n","\n","In our current example, this becomes:\n","\\begin{equation}\n","X_{2\\times 3}  = \\begin{bmatrix} \n","-(\\textbf x^{(1)})^{T} -\\\\-(\\textbf x^{(2)})^{T} - \\end{bmatrix} \\end{equation}\n","\n","\n","\\\\\n","The corresponding feature matrix $\\textbf X $ appears as follows:\n","\\begin{equation}\n","X_{2\\times 3}  = \\begin{bmatrix} \n","3 &2&5\\\\\n","9&4&7\n","\\end{bmatrix} \\end{equation}\n","\n","Here the feature vectors are transposed and represented as rows:\n","* The first row corresponds to the first exam $\\textbf (x^{(1)})^{T}$\n","and \n","* The second row corresponds to the second example $(x^{(2)})^{T}$\n","\n","Once we add the dummy feature, the resulting matrix becomes:\n","\n","\n"],"metadata":{"id":"Rr68YxgpR-fc"}},{"cell_type":"code","source":["import unittest\n","\n","class TestAddDummyFeature(unittest.TestCase):\n","  def test_add_dummy_feature(self):\n","    ''' Test case funciton for add_dummy_feature'''\n","    train_matrix =np.array([[3,2,5],[9,4,7]])\n","    train_matrix_with_dummy_feature = add_dummy_feature(train_matrix)\n","\n","    #test the shape\n","    self.assertEqual(train_matrix_with_dummy_feature.shape,(2,4))\n","\n","    #add contents\n","    np.testing.assert_array_equal(\n","        train_matrix_with_dummy_feature,\n","        np.array([[1,3,2,5],[1,9,4,7]])\n","    )\n","\n","unittest.main(argv=[''],defaultTest='TestAddDummyFeature', verbosity=2, exit=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r9BTnZsvbxQK","outputId":"16edea49-ad35-44fb-e54f-66ae452d0727"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_add_dummy_feature (__main__.TestAddDummyFeature)\n","Test case funciton for add_dummy_feature ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.002s\n","\n","OK\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f8902aaf050>"]},"metadata":{},"execution_count":189}]},{"cell_type":"code","source":["add_dummy_feature(np.array([[3,2],[5,4]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCQR8FE8gZnt","outputId":"fc665b1c-3bbb-486e-afed-2245c7f28672"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 3., 2.],\n","       [1., 5., 4.]])"]},"metadata":{},"execution_count":190}]},{"cell_type":"markdown","source":["Let's preprocess the training set to add the dummy feature."],"metadata":{"id":"J5STlvpkgaw-"}},{"cell_type":"code","source":["print('Before adding the dummy feature:\\n', X_train[:5])\n","print()\n","X_train_with_dummy =add_dummy_feature(X_train)\n","print(\"After adding dummy feature:\\n\",X_train_with_dummy[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TdAFLekqg5Xg","outputId":"dc2de887-f96b-49ac-91b5-68da199f408f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before adding the dummy feature:\n"," [2.3524671  7.49300262 1.89967955 0.66130858 9.34204539]\n","\n","After adding dummy feature:\n"," [[1.         2.3524671 ]\n"," [1.         7.49300262]\n"," [1.         1.89967955]\n"," [1.         0.66130858]\n"," [1.         9.34204539]]\n"]}]},{"cell_type":"markdown","source":["#C2.Model\n","The objective of this colab is to implements model and inference component from linear regression model.   "],"metadata":{"id":"RsiSZZgTDQaJ"}},{"cell_type":"code","source":["#import library for model class"],"metadata":{"id":"jQStCI8S-5c-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Quick recap\n","1. Training data contains features and label that is real number.\n","2. Linear regression model uses linear combination of features to obtain labels. In vectorized form, this can be written as $\\textbf y = \\textbf {Xw}$\n","\n","**Note**\n","* Model is parameterized by its weight vector.\n","* It is described by its mathematical form and weight vector.\n","\n","#Implementation\n","The general vectorized form is as follows:\n","\\begin{equation} \n","\\textbf y_{n\\times 1} =\\textbf X_{n \\times (m+1)} \\textbf w_{(m+1)\\times 1}\n","\\end{equation}\n","\n","where \n","* n is number of examples in dataset (train/test/validation).\n","* m is the number of features.\n","* \\textbf X is a feature matrix contain $(m+1)$ features for $n$ examples along rows.(Notice capital case bold **X** used for matrix)\n","* **w** is weight vector containg $(m+1)$ weights one for each feature. (notice small case bold **w**)\n","* **y** is a label vector containing labels for $n$ examples with shape $(n,)$.\n"],"metadata":{"id":"Tinh7NIwBcNC"}},{"cell_type":"code","source":["def predict(X,w):\n","  \n","  assert X.shape[-1]==w.shape[0], \"X and w don't have compatible dimensions\"\n","  return X @ w #returns the predicted_label vector\n","  "],"metadata":{"id":"oIqkuqBmE8xO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We test this function with the following feature matrix $\\textbf X_{2\\times (3+1)}:$\n","\n","\\begin{equation} \\textbf X_{2 \\times (3+1)} = \\begin{bmatrix} 1&3&2&5\\\\ \n","1&9&4&7\\end{bmatrix} \\end{equation}\n","and weight vector $\\textbf w$\n","\\begin{equation} \\textbf w_{4\\times 1}= \\begin{bmatrix}1\\\\1\\\\1\\\\1 \\end{bmatrix} \n","\\end{equation}\n","\n","Let's perform matrix-vecto multiplication between the feature matrix $\\textbf X$ and a weight vector $\\textbf w$ to obtain labels for all examples:\n","\\begin{eqnarray}  \\textbf y&=&\\textbf {Xw} \\\\\n","&=&\\begin{bmatrix} 1&3&2&5\\\\ 1&9&4&7 \\end{bmatrix} \\times \\begin{bmatrix} 1\\\\1\\\\1\\\\1 \\end{bmatrix}\n","\\\\ &=& \\begin{bmatrix} 1\\times 1+3 \\times1 +2\\times1+5\\times1 \\\\ 1\\times1 + 3\\times1 + 2\\times1 +5\\times1 \\end{bmatrix}\\\\ &=&\\begin{bmatrix} 11\\\\21 \\end{bmatrix} \\end{eqnarray} \n"],"metadata":{"id":"Fu7GY5JuGCyB"}},{"cell_type":"code","source":[" import unittest\n"," class TestPredict(unittest.TestCase):\n","   '''Test case predict frunciton of linear regression'''\n","   def test_predict(self):\n","\n","     '''Test case predict function of linear regression.'''\n","\n","   #set up \n","     train_matrix=np.array([[1,3,2,5],[1,9,4,7]])\n","     weight_vector =np.array([1,1,1,1])\n","     expected_label_vector=np.array([11,21])\n","    #call\n","     predicted_label_vector =predict(train_matrix,weight_vector)\n","\n","    #asserts\n","    #test the shape\n","     self.assertEqual(predicted_label_vector.shape, (2,))\n","    \n","    #and contents\n","     np.testing.assert_array_equal(expected_label_vector,predicted_label_vector) #np.testing.assert_array_equal is a method to test whether two arrays are equal or not\n","\n","unittest.main(argv=[''],defaultTest='TestPredict',verbosity=2, exit=False)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwDn_rGUI1_6","outputId":"4671d8a3-7753-4520-e1a8-5acb6e3c9fd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_predict (__main__.TestPredict)\n","Test case predict function of linear regression. ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.004s\n","\n","OK\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f89034b7850>"]},"metadata":{},"execution_count":194}]},{"cell_type":"markdown","source":["#Demonstration on synthetic dataset\n","# Dataset: $n=100, m=1,[w_0,w_1]=[4,3]$ \n","\n","\n"],"metadata":{"id":"rm4sCWqlveWh"}},{"cell_type":"code","source":["import numpy as np\n","n=100\n","w=np.array([4,3])\n","X=10*np.random.rand(100)\n","X=add_dummy_feature(X)\n","noise=np.random.rand(n,)\n","y= X@w + noise\n","print(X[:5],y[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFwFOj5OwlLN","outputId":"bfbe23d2-4ad3-45cd-e378-2d68b0aba418"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         5.3243538 ]\n"," [1.         5.31360863]\n"," [1.         5.10492993]\n"," [1.         0.65933827]\n"," [1.         9.57700921]] [20.06360044 20.77814227 19.54839841  6.92460721 33.50937447]\n"]}]},{"cell_type":"markdown","source":["Preprocessing: Dummy feature and train-test split\n"],"metadata":{"id":"O1H6S7kK24DQ"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state=42)"],"metadata":{"id":"D9vmmIlW2_B9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since we have not yet trained out model, let's use random weight vector to get predictions from out model for the given dataset.\n"],"metadata":{"id":"fMbO062_4v3W"}},{"cell_type":"code","source":["w_model=np.random.rand(2,) \n","w_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WRxZstj4_QL","outputId":"8655deb2-0da1-4915-e577-5f9ac862c56f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.01720392, 0.41833583])"]},"metadata":{},"execution_count":197}]},{"cell_type":"markdown","source":["Let's compare the prediction with the actual value:\n"],"metadata":{"id":"v7CPu6125Gyl"}},{"cell_type":"code","source":["y_hat=predict(X_train, w_model)\n","y_hat[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2puSY0DL5hMN","outputId":"5b9f5085-41d0-41b5-b752-85e35c38016c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3.29962509, 2.66464273, 1.08505021, 2.64268985, 1.53987223,\n","       3.0497196 , 1.82903615, 2.1377316 , 4.14583361, 2.5883216 ])"]},"metadata":{},"execution_count":198}]},{"cell_type":"markdown","source":["Actual labels are "],"metadata":{"id":"blBqwInO58mn"}},{"cell_type":"code","source":["y_train[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPft0tUk5_t0","outputId":"0208c748-1f93-4b39-e6da-b83f7d2c891d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([27.66997872, 23.5774846 , 12.59906623, 23.80750283, 14.97854162,\n","       26.38935845, 17.50858248, 20.08462024, 34.50063355, 22.49837605])"]},"metadata":{},"execution_count":199}]},{"cell_type":"markdown","source":["#Since we used a random weight vector $\\textbf w$ here, most of the predicted labels do not match the actual labels.\n"],"metadata":{"id":"PT0srM2U6a0D"}},{"cell_type":"markdown","source":["#Comparision of  vectorized and non-vectorized version of model inference"],"metadata":{"id":"osnCFsIl62lP"}},{"cell_type":"code","source":["def non_vectorized_predict(X,w):\n","  ''' Prediction of output label for a given input.\n","\n","      Args:\n","          X: Feature matrix of shape (n,m+1)\n","          w: weight vector of shape (m+1,n)\n","      Returns: \n","          y: Predicted label vector of shape(n,)'''\n","  y=[]\n","  for i in range(0,X.shape[0]):\n","    y_hat_i=0\n","    for j in range(0,X.shape[1]):\n","      y_hat_i+=X[i][j]*w[j]\n","    y.append(y_hat_i)\n","  return np.array(y)\n","\n"],"metadata":{"id":"OOGFvKPp7AMa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's write a test for this function with the same set up as the vectorized implementation.\n"],"metadata":{"id":"Nu0Slkrt60DG"}},{"cell_type":"code","source":["import unittest \n","class TestPredictNonVectorized(unittest.TestCase):\n","  def test_predict_non_vectorized(self):\n","\n","    ''' Test case predict function of linear regression''' \n","      #set up\n","    train_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n","    weight_vector=np.array([1,1,1,1])\n","    expected_label_vector =np.array([11,21])\n","\n","      #call\n","    predicted_label_vector = non_vectorized_predict(train_matrix , weight_vector)\n","      #asserts\n","      #test the shape\n","    self.assertEqual(predicted_label_vector.shape, (2,))\n","      #and contents\n","    np.testing.assert_array_equal(expected_label_vector,predicted_label_vector)\n","unittest.main(argv=[''],defaultTest='TestPredictNonVectorized',verbosity=2, exit=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtFVhkj38ZNi","outputId":"dd204fea-7192-4a7c-b4c7-12da5e66cccf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_predict_non_vectorized (__main__.TestPredictNonVectorized)\n","Test case predict function of linear regression ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.004s\n","\n","OK\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f89034e0e90>"]},"metadata":{},"execution_count":201}]},{"cell_type":"markdown","source":["Let's compare run time of vectorized and non-vectorized versions on dataset with 100 examples."],"metadata":{"id":"gSH_2DLkAd9P"}},{"cell_type":"code","source":["import time\n","start_time=time.time()\n","y_hat_vectorized = predict(X_train,w)\n","end_time=time.time()\n","\n","print(\"Total time incurred in vectorized inference is: %0.6f s\"%(end_time-start_time))\n","start_time=time.time()\n","y_hat_non_vectorized=non_vectorized_predict(X_train,w)\n","end_time=time.time()\n","print(\"Total time incurred in non-vectorized inference is: %0.6f s\"%(end_time - start_time))\n","np.testing.assert_array_equal(y_hat_vectorized, y_hat_non_vectorized)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szR05KaBAn01","outputId":"12f2a771-2ab6-4b3a-a27b-1aad55ed22f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total time incurred in vectorized inference is: 0.000125 s\n","Total time incurred in non-vectorized inference is: 0.000521 s\n"]}]},{"cell_type":"markdown","source":["#DATA generation\n","\n","Let's try to check the difference in their performance on large dataset of 1 million data points.\n"],"metadata":{"id":"pLtX4A5nCg_E"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","def generate_data(n=1000_000):\n","  X=10*np.random.rand(n)\n","  X=add_dummy_feature(X)\n","  noise=np.random.rand(n,)\n","  y= X@w + noise\n","  return X,y \n","\n","def preprocess(X,y):\n","  X_train,y_train,X_test,y_test = train_test_split(X,y, test_size=0.2, random_state=42) \n","  return X_train, y_train,X_test,y_test\n"],"metadata":{"id":"ebAnmO7BCqN-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X,y=generate_data(n=1000_000)\n","X_train, y_train,X_test,y_test = preprocess(X,y) \n","start_time = time.time() \n","y_hat_vectorized = predict(X_train,w) \n","end_time = time.time() \n","\n","print(\"Total time incurred in vectorized inference is : %0.6f s\" %(end_time-start_time))\n","\n","start_time=time.time()\n","y_hat_non_vectorized=non_vectorized_predict(X_train,w) \n","end_time=time.time() \n","print('Total time incurred in non-vectorized inference is: %0.6f s'%(end_time-start_time)) \n","\n","np.testing.assert_array_equal(y_hat_vectorized , y_hat_non_vectorized)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-PUGIpbELXG","outputId":"9ccff212-c1c7-48e7-fc0e-82aadce66eea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total time incurred in vectorized inference is : 0.002769 s\n","Total time incurred in non-vectorized inference is: 2.238766 s\n"]}]},{"cell_type":"markdown","source":["Note that the time required for non-vectorized inference in order of magnitude more than the vectorized inference."],"metadata":{"id":"hh_annZWFseo"}},{"cell_type":"markdown","source":["# C3 $ \\cdot $ \n","The objective of this colab is to implements loss function of linear regression model from scratch.\n","#Quick recap\n","1. Training data contains features and label that is real number.\n","2. Model or inference: $\\textbf y = \\textbf {Xw}$\n","3. Loss function: $ \\begin{equation} J(\\textbf w)= \\frac{1}{2} (\\textbf {Xw}-\\textbf y)^T(\\textbf{Xw}-\\textbf y)\\end{equation}$ \n"],"metadata":{"id":"KymjVQ8XGPh8"}},{"cell_type":"markdown","source":["#Implementation\n","\n","The loss function is calculated as follows:\n","\\begin{equation} J(\\textbf w)= \\frac{1}{2} (\\textbf {Xw}-\\textbf y)^T(\\textbf{Xw}-\\textbf y)\\end{equation}\n","\n","where\n","* $\\textbf X$ is a feature matrix contain $(m+1)$ features for n examples along rows.\n","* $\\textbf w$ is a weight vector containing $(m+1)$ weights one for each feature.\n","* $\\textbf y$ is a label matrix containing labels for $n$ examples in a vector of shape $(n,)$"],"metadata":{"id":"7xBCC0c2JLZM"}},{"cell_type":"code","source":["def loss(X,y,w):\n","    e = (predict(X,w)) - y\n","  \n","    ''' Calculates loss for a model based on known labels.\n","\n","        Args:\n","          X: Feature matrix for given inputs.\n","          y: Output label vecto as predicted by the given mdoel.\n","          w: Weight vector\n","        Returns: \n","          Loss\n","    ''' \n","    return (1/2)*((np.transpose(e))@e)"],"metadata":{"id":"3QsFvBUQJsyV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We will test this function with the following configuration:\n","1. Feature matrix $(\\textbf X)$: \\begin{equation} \\textbf X_{2\\times4}=\\begin{bmatrix} 1&3&2&5\\\\ \n","1&9&4&7\\end{bmatrix}\\end{equation}\n","2. Weight vector$(\\textbf w)$: \\begin{equation} \\textbf w_{4\\times 1}=\\begin{bmatrix} 1\\\\1\\\\1\\\\1 \\end{bmatrix} \\end{equation}\n","3. Label vector $\\textbf y$: \n"," \\begin{equation} \\textbf y_{2\\times 1}=\\begin{bmatrix} 6\\\\11 \\end{bmatrix} \\end{equation}\n","\n"," Let's compute the loss $J(\\textbf w)$ i.e.\n","\n","\\begin{eqnarray} \\textit J \\left(\\begin{bmatrix} 1\\\\1\\\\1\\\\1 \\end{bmatrix}\\right)&=& \\frac{1}{2} (\\textbf {Xw}-\\textbf y)^T(\\textbf{Xw}-\\textbf y)\\\\\n","&=& \\frac{1}{2}\\left(\\begin{bmatrix}1&3&2&5\\\\1&9&4&7 \\end{bmatrix}\\times \\begin{bmatrix} 1\\\\1\\\\1\\\\1\\end{bmatrix}-\\begin{bmatrix} 6\\\\11\\end{bmatrix}\\right)^T \\left(\\begin{bmatrix}1&3&2&5\\\\1&9&4&7 \\end{bmatrix}\\times \\begin{bmatrix} 1\\\\1\\\\1\\\\1\\end{bmatrix}-\\begin{bmatrix} 6\\\\11\\end{bmatrix}\\right)\n","\\\\\n","&=&\\frac{1}{2}\\left(\\begin{bmatrix} 11\\\\21\\end{bmatrix}-\\begin{bmatrix} 6\\\\11\\end{bmatrix}\\right)^T\\left(\\begin{bmatrix} 11\\\\21\\end{bmatrix}-\\begin{bmatrix} 6\\\\11\\end{bmatrix}\\right)\\\\\n","&=&\\frac{1}{2}\\left(\\begin{bmatrix} 5\\\\10\\end{bmatrix}\\right)^T\\left(\\begin{bmatrix} 5\\\\10\\end{bmatrix}\\right)\\\\\n","&=&\\frac{1}{2}\\left(\\begin{bmatrix} 5&10\\end{bmatrix}\\begin{bmatrix} 5\\\\10\\end{bmatrix}\\right)\\\\\n","&=&\\frac{1}{2}\\left(\\begin{bmatrix} 5\\times5+10\\times10\\end{bmatrix}\\right)\\\\\n","&=&\\frac{1}{2}[125]=[62.5]\n","\\end{eqnarray} \n"],"metadata":{"id":"DurNPnYSLM7q"}},{"cell_type":"code","source":["import unittest\n","class TestLossFunction(unittest.TestCase):\n","\n","  def testt_loss_function(self):\n","\n","      '''Test case for loss function of linear regression'''\n","      #set up \n","      feature_matrix=np.array([[1,3,2,5],[1,9,4,7]])\n","      weight_vector = np.array([1,1,1,1]) \n","      label_vector =np.array([6,11])\n","      expected_loss=np.array([62.5])\n","      #call\n","      loss_value = loss(feature_matrix,label_vector,weight_vector)\n","      #asserts\n","      #test the shape\n","      self.assertEqual(loss_value.shape, ())\n","      #and contents\n","      np.testing.assert_array_equal(expected_loss, loss_value)\n","unittest.main(argv=[''], defaultTest='TestLossFunction',verbosity=2,exit=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N0d7ftL8LUtk","outputId":"bf1b66c9-0040-4741-c6a9-08f8e2c5d014"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["testt_loss_function (__main__.TestLossFunction)\n","Test case for loss function of linear regression ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.002s\n","\n","OK\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f89035460d0>"]},"metadata":{},"execution_count":206}]},{"cell_type":"markdown","source":["Since we have not yet trained our model, let's use a random weight vector to calculate loss for linear regression model with single feature on synthetic dataset.\n"],"metadata":{"id":"Chop7vfzXN39"}},{"cell_type":"code","source":["w=np.random.rand(2,)\n"],"metadata":{"id":"zbxHd5axXdsE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Demonstration on synthetic dataset\n"],"metadata":{"id":"64yi2LREXmBs"}},{"cell_type":"markdown","source":["#Generating training data\n","#Preprocessing: Dummy feature and train_test split"],"metadata":{"id":"ZNZ61KOSXscK"}},{"cell_type":"code","source":["X,y=generate_data(100) #y =4+3*X_1 + noise\n","X_train,X_test,y_train,y_test=preprocess(X,y)"],"metadata":{"id":"i9-AO5yjX4sr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize error for each data point"],"metadata":{"id":"v7j6opxxYpOw"}},{"cell_type":"code","source":["#visualize_loss_for_single_feature_model(X_train,y_train,w) # missing code search in week 1 of MLP"],"metadata":{"id":"7-ipEDSDY_Y2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Excercise\n","1. Plot loss as shown for an example above for different weight vectors. Which weight vecto has the best fitment?\n","2. Implement non-vectorized version of loss function and compare time between vectorized and non-vectorized version on datasets of different sizes.\n","    1. Implement non-vectorized version of loss computation ```loss_non_vectorized```:\n","      * for each poinnt, calculate the predicted value.\n","      * Calculate the square of difference between predicted and actual values.\n","      * Sum up these values across all training examples.\n","    2. Generate datasets with ``` generate_data``` function with  n=\\{100,1000,10000,1000_000\\}and for each dataset:\n","      * Calculate the loss with vectorized ```loss``` and non-vectorized versions```loss_non_vectorized```.\n","      * Record time before and after calling this function and calculate the time needed to compute the loss with these funcitons.\n","\n","    3. Compare the times and note down your observations.\n","\n"],"metadata":{"id":"9F3LLZQMYv0P"}},{"cell_type":"code","source":["'''import numpy as np\n","X=np.array([[1,2,2,1],[1,1,3,2]])\n","y=np.array([3,5])\n","w=np.array([1,2,3,4])\n","loss(X,y,2*w)''' #activity question"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"jp3lj42XekWO","outputId":"6314ebda-6677-4eaf-9f5d-e60e9534179b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'import numpy as np\\nX=np.array([[1,2,2,1],[1,1,3,2]])\\ny=np.array([3,5])\\nw=np.array([1,2,3,4])\\nloss(X,y,2*w)'"]},"metadata":{},"execution_count":210}]},{"cell_type":"markdown","source":["#C4 Optimization\n","The objective of this colab is to implement optimization component of linear regression model.\n","It is implemented with one of the following two methods:\n","* Normal equation method, that sets the partial derivative of the loss function w.r.t. weight vector to 0 and solves the resulting equation to obtain the weight vector.\n","*Gradient descent method, that iteratively adjusts the weight vector based on the learning rate and the gradient of loss function at the current weight vector.\n"],"metadata":{"id":"r5sDpqKcgwEH"}},{"cell_type":"markdown","source":["Normal equation\n","The weight vector is estimated by matrix multiplication of pseudo-inverse of feature matrix and the label vector.\n","The vectorized implementation is fairly straight forward.\n","* We make use of ```np.linalg.pinv``` for calculating pseudoinverse of the feature matrix.\n"," "],"metadata":{"id":"_AwfK9PIhsfY"}},{"cell_type":"code","source":["def normal_equation(X,y):\n","\n","  ''' Estimates paramenters of the linear regression model with normal equation.\n","      \n","      Args:\n","         X: Feature matrix for given inputs.\n","         y: Actual label vector.\n","      Returns:\n","         weight vector'''\n","  return np.linalg.pinv(X) @ y\n","  "],"metadata":{"id":"Fzzf3CdCisv_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We test this function with the generated training set whose vector is known to use. \n","* We set up the test with feature matrix, label vector and the expected weight vectors.\n","* Next we estimate the weight vector with ```normal_equation``` function.\n","* We test(a) shape and (b) match between expected and estimated weight vectors.\n"],"metadata":{"id":"AinX4f1VjKst"}},{"cell_type":"code","source":["import unittest \n","class TestNormalEquation(unittest.TestCase):\n","\n","  def test_normal_equation(self):\n","      ''' Test case for weight estimation for linear regression with normal equation method.'''\n","      #setup\n","      feature_matrix =  X_train\n","      label_vector = y_train \n","      expected_weight_vector = np.array([4.,3.])\n","\n","      #call\n","\n","      estimated_weight_vector = normal_equation(feature_matrix, label_vector)\n","\n","      #asserts\n","      # test the shape\n","      self.assertEqual(estimated_weight_vector.shape, (2, ))\n","      #and contents\n","      np.testing.assert_array_almost_equal(estimated_weight_vector , expected_weight_vector , decimal=0)\n","      \n","unittest.main(argv=[''],defaultTest='TestNormalEquation',verbosity=2,exit=False)      "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1pgJ88ijyTP","outputId":"7db63925-11d0-4174-8ac5-315e1cf9b1a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_normal_equation (__main__.TestNormalEquation)\n","Test case for weight estimation for linear regression with normal equation method. ... FAIL\n","\n","======================================================================\n","FAIL: test_normal_equation (__main__.TestNormalEquation)\n","Test case for weight estimation for linear regression with normal equation method.\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"<ipython-input-212-d17f8b0000f2>\", line 19, in test_normal_equation\n","    np.testing.assert_array_almost_equal(estimated_weight_vector , expected_weight_vector , decimal=0)\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py\", line 1044, in assert_array_almost_equal\n","    precision=decimal)\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\n","    raise AssertionError(msg)\n","AssertionError: \n","Arrays are not almost equal to 0 decimals\n","\n","Mismatched elements: 2 / 2 (100%)\n","Max absolute difference: 3.10651808\n","Max relative difference: 0.86370408\n"," x: array([1., 0.])\n"," y: array([4., 3.])\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.007s\n","\n","FAILED (failures=1)\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f89034ea710>"]},"metadata":{},"execution_count":212}]},{"cell_type":"markdown","source":["#Gradient descent (GD):\n","\n","GD is implemented as follows:\n","* Randomly initialize $\\textbf w$ to $\\textbf 0$.\n","* Iterate until convergence:\n","    * Calculate partial derivative of loss w.r.t weight vector.\n","    * Calculate new values of weights.\n","    * Update weights to new values $\\textit {simultaneously}$. \n","We use number of epochs as a convergence criteria in this implementation.\n","\n","#Partial derivative of loss function\n","\n","Let's first implement a function to calculate partial derivative of loss funciton, which is obtained with the following equation.\n","\\begin{equation} \\frac{∂}{∂ \\textbf w}  J(\\textbf w)= \\textbf X^T(\\textbf{Xw}-\\textbf y)\\end{equation} \n","\n","The multiplication of transpose of feature matrix with the difference of predicted and actual label  vectors.\n"],"metadata":{"id":"uXPu_NrRnkle"}},{"cell_type":"code","source":["def calculate_gradient(X,y,w):\n","  ''' Calculates gradients of loss function w.r.t weight vector on training set.\n","      Arguments:\n","            X: Features matrix for training data.\n","            y: Label vector for training data.\n","            w: Weight vector\n","      Retruns:\n","            A vector of gradients.'''\n","  return np.transpose(X)@(predict(X,w)-y)\n","\n","\n","  "],"metadata":{"id":"Ic5j-5FYrR-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's write  a test case for gradient calculation wtih the following setup:\n","1. Feature matrix $(\\mathbf X)$\n","\\begin{equation} \\textbf X_{2 \\times 4} = \\begin{bmatrix} 1&3&2&5\\\\ \n","1&9&4&7\\end{bmatrix} \\end{equation}\n","2. weight vector $\\textbf w$\n","\\begin{equation} \\textbf w_{4\\times 1}= \\begin{bmatrix}1\\\\1\\\\1\\\\1 \\end{bmatrix} \n","\\end{equation}\n","3. Label Vector $y$: \\begin{equation} y_{2 \\times 1} =  \\begin{bmatrix} 6\\\\11   \\end{bmatrix} \\end{equation}\n","\n","\n","Let's compute the partial derivative of loss $ J(\\mathbf w) $ i.e\n","\\begin{eqnarray} \\frac{∂}{∂ \\textbf w}  J(\\textbf w)&=& \\textbf X^T(\\textbf{Xw}-\\textbf y) \\\\ &=&\\begin{bmatrix} 15\\\\105\\\\50\\\\95 \\end{bmatrix}\n","\\end{eqnarray}"],"metadata":{"id":"sGux8LgtW6Eb"}},{"cell_type":"code","source":["class TestCalculateGradient(unittest.TestCase): \n","  def test_calculate_gradient(self):\n","    ''' Test case for gradient calculation. ''' \n","    #set up \n","    feature_matrix = np.array([[1,3,2,5], [1,9,4,7]])\n","    weight_vector = np.array([1,1,1,1])\n","    label_vector = np.array([6,11]) \n","    expected_grad = np.array([15,105,50,95])\n","\n","    #call \n","    grad = calculate_gradient(feature_matrix, label_vector , weight_vector) \n","\n","    #asserts \n","    #test the shape \n","    self.assertEqual(grad.shape, (4, ))\n","    #and contents \n","    np.testing.assert_array_almost_equal(expected_grad, grad, decimal =0)\n","unittest.main(argv=[''], defaultTest='TestCalculateGradient', verbosity=2 , exit=False)"],"metadata":{"id":"Pdokw826NIjY","outputId":"b2cff51d-e87a-4245-8c82-3e2769003d2e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_calculate_gradient (__main__.TestCalculateGradient)\n","Test case for gradient calculation. ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.002s\n","\n","OK\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f8902aafc90>"]},"metadata":{},"execution_count":214}]},{"cell_type":"markdown","source":["#Weight updates\n","Next let's implement the weight update part:\n","* We obtain the new weight from the old one by substracting gradient weighted by the learning rate.\n"],"metadata":{"id":"lpN1iSv2slrf"}},{"cell_type":"code","source":["def update_weights(w,grad,lr):\n","  ''' Updates the weights based on the gradient of loss function.\n","  Weight updates are carried out with the following formula: \n","      w_new = w_old - lr*grad # lr is learning rate \n","  Args:\n","      1. w: weight vector\n","      2. grad: gradient of loss wrt w\n","      3. lr: learning rate (alpha)\n","  Retruns:\n","      Updated weight vector\n","      '''\n","  return (w - lr*grad)"],"metadata":{"id":"kxp6qzDktE5F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's workout a weight update for :\n","1. Weight vector$(\\textbf w)$: \\begin{equation} \\textbf w_{4\\times 1}=\\begin{bmatrix} 1\\\\1\\\\1\\\\1 \\end{bmatrix} \\end{equation} \n","2. grad vector$(\\textbf g)$: \\begin{equation} \\textbf g_{4\\times 1}=\\begin{bmatrix} 15\\\\105\\\\50\\\\95 \\end{bmatrix} \\end{equation}\n","3. Learning rate = 0.001\n","\n","The updated weights are given by:\n","\\begin{eqnarray}  \\textbf w:&=&\\textbf w^{(old)} -\\alpha \\mathbf g \\\\\n","&=&\\begin{bmatrix} 1\\\\1\\\\1\\\\1\\\\  \\end{bmatrix} -0.001\\times \\begin{bmatrix} 15\\\\105\\\\50\\\\95 \\end{bmatrix}\n","\\\\ &=& \\begin{bmatrix} 1-0.015\\\\1-0.105\\\\1-0.05\\\\1-0.095 \\end{bmatrix} \\\\&=&\\begin{bmatrix} 0.985\\\\0.895\\\\0.95\\\\0.095 \\end{bmatrix}\\end{eqnarray} \n"],"metadata":{"id":"gUlCQf1vEoAZ"}},{"cell_type":"code","source":["class TestUpdateWeights(unittest.TestCase):\n","  def test_update_weights(self):\n","    ''' Test case for weight update in GD'''\n","    #set up \n","    weight_vector = np.array([1,1,1,1])\n","    grad_vector = np.array([15,105,50,95]) \n","    lr =0.001\n","    expected_w_new = np.array([0.985,0.895,0.95,0.905])\n","\n","    #call\n","    w_new = update_weights(weight_vector, grad_vector, lr)\n","\n","    #asserts \n","    #test the shape \n","    self.assertEqual (expected_w_new.shape, (4,))\n","\n","    #and contents \n","    np.testing.assert_array_almost_equal(expected_w_new, w_new, decimal =1)\n","unittest.main(argv=[''], defaultTest='TestUpdateWeights', verbosity=2, exit=False) \n"],"metadata":{"id":"Cut3-tNMHuyx","outputId":"26e17e54-9d9a-4dd6-a4cc-17ac3b7698d2","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_update_weights (__main__.TestUpdateWeights)\n","Test case for weight update in GD ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.003s\n","\n","OK\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f89034efc10>"]},"metadata":{},"execution_count":216}]},{"cell_type":"markdown","source":["copy later at 7:51 in optimization video"],"metadata":{"id":"ITnjrIGSuErA"}},{"cell_type":"code","source":[" def gradient_descent(X:np.ndarray,y:np.ndarray,lr:float, num_epochs:int):\n","    \n","    w_all=[]\n","    err_all=[]\n","    w =np.zeros((X.shape[1])) \n","    print()\n","    for i in np.arange(0, num_epochs):\n","      w_all.append(w)\n","      err_all.append(loss(X,y,w))\n","      dJdW = calculate_gradient(X,y,w)\n","\n","      if (i%100)==0:\n","        print('Iteration #:%d,loss: %4.2f'%(i,err_all[-1]))\n","      w = update_weights (w , dJdW , lr) \n","    return w, err_all, w_all "],"metadata":{"id":"ryEoRunTuJgk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to test this function, we will use the synthetic training data that was generated earlier. We know the acutal weigths, that can be compared against the weights obtained from gradient descent procedure."],"metadata":{"id":"Fra-sL_sUR3t"}},{"cell_type":"code","source":["class TestGradientDescent(unittest.TestCase):\n","  def test_gradient_descent(self):\n","    ''' Test case for weight update in GD'''\n","    \n","    ''' Test case for gradient calculation. ''' \n","    #set up \n","    feature_matrix = X_train \n","    label_vector = y_train \n","    expected_weight = np.array([4.,3.])\n","\n","    #call \n","    w, err_all, w_all = gradient_descent(feature_matrix,label_vector , lr=0.001 , num_epochs=2000)\n","    #asserts \n","    #test the shape \n","    self.assertEqual(w.shape, (2, ))\n","    #and contents \n","    np.testing.assert_array_almost_equal(expected_weight, w, decimal =0)\n","unittest.main(argv=[''], defaultTest='TestGradientDescent', verbosity=2 , exit=False)"],"metadata":{"id":"gKXSu0ECUlHF","outputId":"e268c619-b689-4124-c7f9-c42228632828","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_gradient_descent (__main__.TestGradientDescent)\n","Test case for weight update in GD ... /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in matmul\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in matmul\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in subtract\n","  if sys.path[0] == '':\n"]},{"output_type":"stream","name":"stdout","text":["\n","Iteration #:0,loss: 415.36\n","Iteration #:100,loss: 5025311508723060321579174842766845714698092641265909760.00\n","Iteration #:200,loss: 62404942882339183872006932104379009214109581880068612265271479141074662449854648692997051015104797308092416.00\n","Iteration #:300,loss: 774952336663718024971888634865074692219733269949243889784198451494796375476832860162205567496682298317001370337993721543451441661030991105534741320777756311552.00\n","Iteration #:400,loss: 9623454431051504511972790010869723947127802551115545629633738874270143459961223488743224024369316871908450169586009447505866462312902055709832948725196747589961779343945806239289409603831237862058524345028837376.00\n","Iteration #:500,loss: 119505253168508164269176655613953195720373563724540164605933747811335308275889896467429859829411384091061804610688215005170378740681286011191256144306187478914186690385980495614178240535517000318888703866967332238511831613274160472469416824362833154455206100992000.00\n","Iteration #:600,loss:  inf\n","Iteration #:700,loss:  inf\n","Iteration #:800,loss:  inf\n","Iteration #:900,loss:  inf\n","Iteration #:1000,loss:  inf\n","Iteration #:1100,loss:  inf\n","Iteration #:1200,loss:  nan\n","Iteration #:1300,loss:  nan\n","Iteration #:1400,loss:  nan\n","Iteration #:1500,loss:  nan\n","Iteration #:1600,loss:  nan\n","Iteration #:1700,loss:  nan\n","Iteration #:1800,loss:  nan\n","Iteration #:1900,loss:  nan\n"]},{"output_type":"stream","name":"stderr","text":["FAIL\n","\n","======================================================================\n","FAIL: test_gradient_descent (__main__.TestGradientDescent)\n","Test case for weight update in GD\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"<ipython-input-218-1907ca16f20e>\", line 17, in test_gradient_descent\n","    np.testing.assert_array_almost_equal(expected_weight, w, decimal =0)\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py\", line 1044, in assert_array_almost_equal\n","    precision=decimal)\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py\", line 764, in assert_array_compare\n","    flagged = func_assert_same_pos(x, y, func=isnan, hasval='nan')\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py\", line 740, in func_assert_same_pos\n","    raise AssertionError(msg)\n","AssertionError: \n","Arrays are not almost equal to 0 decimals\n","\n","x and y nan location mismatch:\n"," x: array([4., 3.])\n"," y: array([nan, nan])\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.050s\n","\n","FAILED (failures=1)\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f890351b5d0>"]},"metadata":{},"execution_count":218}]},{"cell_type":"code","source":["'''import numpy as np\n","w1 = 5 \n","w0 = 6 \n","n = 200\n","X=np.random.rand(n,)\n","y=w0+w1*X+np.random.randn(n,)\n","print('X sh',X.shape)\n","print('ysha',y.shape)\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=36)\n","print(X_train.shape,y_train.shape,y_test.shape,X_test.shape)'''\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"rLrdI5BHD_XT","outputId":"0ce03023-d9c2-4c0a-f2b9-1078112534a2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"import numpy as np\\nw1 = 5 \\nw0 = 6 \\nn = 200\\nX=np.random.rand(n,)\\ny=w0+w1*X+np.random.randn(n,)\\nprint('X sh',X.shape)\\nprint('ysha',y.shape)\\nfrom sklearn.model_selection import train_test_split\\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=36)\\nprint(X_train.shape,y_train.shape,y_test.shape,X_test.shape)\""]},"metadata":{},"execution_count":219}]},{"cell_type":"code","source":["'''import numpy as np\n","np.arange(12).reshape(-1,4)'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wSKh7zOcvrsr","outputId":"cf69f387-861f-4ea9-dd60-f6b36fd7efaf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'import numpy as np\\nnp.arange(12).reshape(-1,4)'"]},"metadata":{},"execution_count":220}]},{"cell_type":"code","source":["'''#@ [Plot learning curves]\n","def plot_learning_curves(err_all):\n","  plt.plot(err_all)\n","  plt.xlabel('iteration #')\n","  plt.ylabel('Loss: $J(\\mathbf {w})$')'''"],"metadata":{"id":"kf2C8xE8CC65","outputId":"820a10c6-5586-4a8f-e54d-0253fbcbcb27","colab":{"base_uri":"https://localhost:8080/","height":35}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"#@ [Plot learning curves]\\ndef plot_learning_curves(err_all):\\n  plt.plot(err_all)\\n  plt.xlabel('iteration #')\\n  plt.ylabel('Loss: $J(\\\\mathbf {w})$')\""]},"metadata":{},"execution_count":221}]},{"cell_type":"code","source":["'''from sklearn.model_selection import train_test_split\n","def preprocess(X,y):\n","  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state=42) \n","  return X_train, X_test ,y_train, y_test\n","'''\n","X ,y = generate_data(100)\n","X_train ,X_test, y_tain,t_test = preprocess(X,y)\n","\n"],"metadata":{"id":"xE-sqYKxCoEX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Mini-Batch gradient descent (MBGD)\n","\n","The key idea here to perform weight updates by computing gradient on batches of small number of examples.\n"],"metadata":{"id":"Cg4lYvmxC0fW"}},{"cell_type":"code","source":["t0,t1 = 200,100000 \n","def learning_schedule(t):\n","  return t0/(t+t1) \n"],"metadata":{"id":"LF0zSrrrDBO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mini_batch_gd(X:np.ndarray, y:np.ndarray,num_iters:int, minibatch_size:int):\n","\n","  ''' Estimates parameters of linear regression model through gradient descent.\n","  Args: \n","  X: feature matrix for training data\n","  y: label vector  for training data\n","  num_iters: number of iterations.\n","  Returns:\n","    weight vector: Final weight vector\n","    Error vector across different iterations\n","    weight vectors across different iterrations\n","  '''\n","  w_all =[]# all parameters across iterations.\n","  err_all = [] # error across iterations \n","  #parameter vector initialized to [0,0]\n","\n","  w = np.zeros((X.shape[1]))\n","  \n","  t = 0 \n","  for epoch in range(num_iters):\n","\n","    shuffled_indices = np.random.permutation(X.shape[0]) \n","    X_shuffled = X[shuffled_indices]\n","    y_shuffled = y[shuffled_indices]\n","\n","    for i in range(0, X.shape[0], minibatch_size):\n","      t+=1 \n","      xi = X_shuffled[i,i + minibatch_size]\n","\n","      yi = y_shuffled[i,i + minibatch_size]\n","\n","      err_all.append(loss(xi,yi,w))\n","\n","      gradients = 2/minibatch_size * calculate_gradient(xi,yi,w)\n","      lr = learning_schedule(t)\n","\n","      w = update_weights(w,gradients, lr)\n","      w_all.append(w)\n","  return w, err_all, w_all"],"metadata":{"id":"fgRGijApDOLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import unittest \n","class TestMiniBatchGradientDescent(unittest.TestCase):\n","  \n","  def test_minibatch_gd(self):\n","\n","    #set up \n","    feature_matrix = X_train \n","    label_vector = y_train \n","    expected_weights = np.array([4.,3.])\n","\n","\n","    #call \n","    w, err_all, w_all = mini_batch_gd(feature_matrix , label_vector ,200 , 8)\n","\n","    #asserts \n","    #test the shape \n","    self.assertEqual(w.shape ,(2,))\n","\n","    #and contents \n","    np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n","\n","unittest.main(argv=[''],defaultTest='TestMiniBatchGradientDescent',verbosity=2,exit=False)\n"],"metadata":{"id":"HOxsr4-9pV1y","outputId":"5e784b5a-59c9-4caa-922e-67bd7feb7bc9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["test_minibatch_gd (__main__.TestMiniBatchGradientDescent) ... ERROR\n","\n","======================================================================\n","ERROR: test_minibatch_gd (__main__.TestMiniBatchGradientDescent)\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"<ipython-input-225-1e1dcffcd3dd>\", line 13, in test_minibatch_gd\n","    w, err_all, w_all = mini_batch_gd(feature_matrix , label_vector ,200 , 8)\n","  File \"<ipython-input-224-93f6c9f3ed76>\", line 28, in mini_batch_gd\n","    xi = X_shuffled[i,i + minibatch_size]\n","IndexError: index 8 is out of bounds for axis 1 with size 2\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.003s\n","\n","FAILED (errors=1)\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.main.TestProgram at 0x7f89032898d0>"]},"metadata":{},"execution_count":225}]},{"cell_type":"code","source":["\n","X=np.arange(60).reshape(15,4)\n","X_shuffled = np.random.permutation(X.shape[0])\n","X_shuffled\n"],"metadata":{"id":"WSZ-VFDvmXM6","outputId":"0797ef0b-c993-4a3b-ac02-9be818e934f1","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([14, 13, 10,  3,  9, 12,  5,  1,  6,  8,  0,  7,  2,  4, 11])"]},"metadata":{},"execution_count":226}]}]}